In this example validation to check  the
length of the given column in the input data is less than 20 characters:

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from pyspark.context import SparkContext
from pyspark.sql.functions import *
from pyspark.sql.types import *


# Define Glue job arguments
args = getResolvedOptions(sys.argv, ['GLUE_JOB_NAME', 'DB_NAME', 'RAW_TABLE_NAME', 'OUTPUT_TABLE', 'S3_OUTPUT_PATH'])

# Create Glue and Spark contexts here
glueContext = GlueContext(SparkContext.getOrCreate())
spark = glueContext.spark_session

# here Read data from Glue database table
database = args['DB_NAME']
input_table = args['RAW_TABLE_NAME']
source = glueContext.create_dynamic_frame.from_catalog(database=database, table_name=input_table)

# Perform ETL transformations
# Example: Convert a column of strings to uppercase
transformed = ApplyMapping.apply(frame=source, mappings=[("name", "string", "name", "string")])
transformed = transformed.toDF()

# Validate data
transformed = transformed.withColumn('name_length', length(transformed['name']))
invalid_data = transformed.filter(transformed['name_length'] >= 20)
if invalid_data.count() > 0:
    raise ValueError(f"{invalid_data.count()} rows of input data have 'name' column with length greater than or equal to 20")

# Perform additional ETL transformations
transformed = transformed.withColumn('name_uppercase', upper(transformed['name']))
transformed = transformed.drop('name', 'name_length')

# Write output to Glue table
output_table = args['OUTPUT_TABLE']
glueContext.write_dynamic_frame.from_options(frame=DynamicFrame.fromDF(transformed, glueContext, output_table), connection_type="gluecatalog", connection_options={"database": database, "table_name": output_table})

# Write output to S3 bucket
s3_output_path = args['S3_OUTPUT_PATH']
transformed.write.mode('overwrite').parquet(s3_output_path)
